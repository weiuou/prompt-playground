\section{Related Work}
The gradient-free prompt optimization for closed-source LLMs typically contains two phases: initialization and iterative optimization steps, where the optimization step consists of expansion and selection stages. 

\paragraph{Initialization.}
The prompt initialization for optimization can be achieved manually or autonomously.
Manual initialization often entails professional machine learning engineers formulating prompts, as delineated in \cite{Pryzant2023AutomaticPO}. Concurrently, works such as \cite{Guo2023ConnectingLL}, \cite{Pan2023PlumPL}, and \cite{Wang2023PromptAgentSP} utilize existing manual prompts as the foundational set to harness human creativity. 
In contrast, automated initialization leverages the power of LLM generation, which is exemplified by \cite{Zhang2023AutoInstructAI}, generating prompts from few-shot exemplars and a rudimentary description, and \cite{Zhou2022LargeLM}, fabricating prompts based on meta-prompts and illustrative input-output examples.
Our method belongs to the automated initialization, improving the initial prompt generation for acceleration. 

\paragraph{Optimization.}
The optimization step is achieved by expanding prompt candidates by modifying from the initial prompt and selecting the better candidates for the next iteration. 
The expansion stage can be executed through rephrasing, as in \cite{Zhou2022LargeLM}, where high-scoring prompts undergo evolution akin to a Monte Carlo search methodology, or through heuristic algorithms that automatically revise prompts, as in \cite{Guo2023ConnectingLL} and \cite{Pan2023PlumPL}. More complex regeneration strategies are employed by works like \cite{Wang2023PromptAgentSP}, where the optimizer LLM progressively expands prompts based on task delineations and historical iterations. 
The expansion can also be implemented leveraging an open-source LLM \cite{Lin2023UseYI, Chen2023InstructZeroEI}. 
Reinforcement learning-based methods have also been adopted for prompt modification \cite{diao2023blackbox}. 
Moreover, the granularity of prompt modification exhibits variation across studies. Heuristic-based methods and \cite{Hsieh2023AutomaticEO} work operate at the word/token granularity, while classical optimization algorithms like \cite{Pryzant2023AutomaticPO, Zhou2022LargeLM} consider the entire prompt. 
The selection stage generally utilized the performance of the prompt on a held-out validation set \cite{Pryzant2023AutomaticPO, Zhou2022LargeLM, Wang2023PromptAgentSP}, while recent work also explores human preference feedback \cite{Lin2024PromptOW} or score feedback from other LLMs \cite{yang2024large}. 