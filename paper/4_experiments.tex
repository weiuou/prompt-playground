\section{Experiments}
In this section, we begin by detailing datasets, baselines, and the implementation of the experiments. Following this, we conduct comprehensive and controlled experiments on our method.


\subsection{Experimental Settings}

\paragraph{Datasets.}
Our experiments are first conducted on general natural language understanding tasks across four datasets to validate our method, specifically focusing on sentiment classification (SST-2 \cite{Socher2013RecursiveDM}), topic classification (AG’s
News \cite{Zhang2015CharacterlevelCN}, TREC \cite{Voorhees2000BuildingAQ}) and subjectivity classification (Subj \cite{Pang2004ASE}). 
Then we perform our approach to the challenging BBH tasks \cite{Suzgun2022ChallengingBT}, 
which include manually provided few-shot Chain-of-Thought (CoT) prompts containing task descriptions and demonstrations.


\paragraph{Baselines.}
We compare our method with three popular prompt optimization methods for zero-shot black-box prompting and the well-crafted prompts manually provided in BBH tasks:
    \textbf{APO} \cite{Pryzant2023AutomaticPO}: 
    Generating natural language ``gradients'' to criticize and improve the current prompts.
    \textbf{APE} \cite{Zhou2022LargeLM}: Proposing both a naive and an iterative Monte Carlo search methods to approximate the solution to the prompt optimization problem.
    \textbf{PromptAgent} \cite{Wang2023PromptAgentSP}: 
    Automating expert-level prompt generation by treating it as a strategic planning problem using Monte Carlo tree search and error feedback to refine and optimize prompts.
    \textbf{Manual Prompt} \cite{Suzgun2022ChallengingBT}: The few-shot CoT version of human-designed prompts with teaching examples developed in BBH tasks.


\paragraph{Implementation Details.}
In line with \cite{Wang2023PromptAgentSP}, since BBH tasks lack an official train-test split, we shuffle the data and allocate approximately half for testing. The rest is used for training, prompt generation, and optimization. For datasets with predefined test sets, we use those directly.

Unless otherwise stated, we evaluate performance (\ie accuracy) on GPT-3.5-Turbo using the OpenAI API\footnote[1]{\url{https://chat.openai.com/}} (currently gpt-3.5-turbo-0125) in a zero-shot prompt setting. The temperature is set to 0 for prediction and 0.5 for prompt generation to enhance diversity.
To accelerate prompt optimization, we limit the maximum optimization steps to \textbf{four} for all methods, while keeping other baseline parameters and settings at default. 
At the beginning of prompt initialization, eight exemplars are obtained by concatenating unique input-output pairs from the shuffled training data until the desired amount is reached, ensuring no duplicate inputs.
Due to limited computational resources, our approach generates and optimizes only one initial prompt.
By default, we set $H_F=0.3$, $H_V=0.1$, $\alpha=0.4$, and $\eta=0.055$ in Algorithm ~\ref{algo} to accelerate the optimization phase.


\subsection{Main Results \& Analysis}

\begin{table}[!th]
  \centering
  \footnotesize
  \tabcolsep=2pt
  \begin{tabular}{l|c|cccc}
    \toprule
    & \textbf{Few-shot} & \multicolumn{4}{c}{\textbf{Zero-shot}} \\
    \midrule
    \textbf{Task} & \textbf{Manual} & \textbf{APO} & \textbf{APE} & \textbf{PA} & \textbf{Ours} \\
    \midrule
    \textbf{SST-2} & \slash & 0.89 & \underline{0.92} & 0.443 & \textbf{0.978} \\
    \textbf{AG’s News} & \slash & \underline{0.88} & 0.819 & 0.785 & \textbf{0.928} \\
    \textbf{TREC} & \slash & \textbf{0.795} & 0.513 & 0.687 & \underline{0.785} \\
    \textbf{Subj} & \slash & \underline{0.64} & 0.593 & 0.494 & \textbf{0.72} \\
    \midrule
    \textbf{Logical Five} & 0.388 & 0.392 & 0.404 & \underline{0.443} & \textbf{0.48} \\
    \textbf{Hyperbaton} & 0.744 & 0.808 & \underline{0.865} & 0.823 & \textbf{0.88} \\
    \textbf{Disambiguation} & 0.580 & 0.688 & 0.645 & \underline{0.696} & \textbf{0.74} \\
    \textbf{Salient Translation} & 0.544 & 0.456 & \underline{0.538} & 0.468 & \textbf{0.548} \\
    \midrule
    \textbf{Avg.} & 0.564 & 0.694 & 0.662 & 0.605 & \textbf{0.757} \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy on eight tasks on GPT-3.5-Turbo. PA indicates PromptAgent. Bold and underlined text indicate the best and second-best results, respectively.}
  \label{tab:main}
\end{table}


\begin{figure*}[!t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
    \includegraphics[width=0.9\textwidth]{figs/comparison_gpt_1.pdf}
    \caption{Performance (accuracy) over 4 steps across 8 tasks on GPT-3.5-Turbo.}
    \label{fig:gpt-all}
\end{figure*}

\paragraph{Overall Results.}
Table~\ref{tab:main} demonstrates the effectiveness of our accelerated dual-phase approach across 8 NLP tasks compared to classic prompt optimization methods. Our method significantly outperforms all baselines, achieving an average improvement of approximately \textbf{10.7\%} over APO, \textbf{16.4\%} over APE, and \textbf{29.7\%} over PromptAgent across the given tasks.

Our method also surpasses few-shot CoT human-crafted prompts with an approximately \textbf{17.6\%} average improvement on selected BBH tasks, indicating its ability to produce high-quality prompts that enhance the black-box LLM’s capabilities in logical deduction, grammar, language understanding, and multilingual tasks without teaching examples.



\paragraph{Analysis.}
To understand this result, we analyzed the prompt expansion and acceptance processes:
In prompt expansion, our method leverages past experience, filters out unnecessary optimization attempts, and collects undesired revisions. This contrasts with baseline methods that inefficiently explore prompt space and underutilize past iterations. APE lacks reflection on past iterations, slowing its Monte Carlo-based search. APO uses error feedback to guide beam search but is slowed by evaluating many paths. PromptAgent’s Monte Carlo Search Tree explores prompt optimization through simulations, but limited steps lead to suboptimal results.

In the acceptance process, inspired by the EXP3 algorithm, our method uses weighted sentences and modifications to enhance prompt quality, making it superior in identifying promising candidates and optimizing directions.


\paragraph{Convergence Analysis.}
To evaluate our method’s convergence within four steps compared to others, we examine how quickly each method achieves peak performance across datasets. Figure \ref{fig:gpt-all} shows the performance (accuracy) variation of four prompt optimization methods across eight datasets, with each subfigure representing a different dataset.
While APO, APE, and PromptAgent experience fluctuations or plateau at lower accuracy, our method demonstrates the fastest convergence across most datasets, often reaching near-peak performance within the first two steps. This rapid convergence highlights our method’s efficiency in optimizing prompts quickly and effectively, making it promising for tasks requiring prompt optimization within a few steps.

\subsection{Ablation Study}

We conduct several ablation experiments to assess the efficacy of our method.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.46\textwidth]{figs/comparison_ymc_init.pdf}
    \caption{Results on GPT-3.5-Turbo with different initial prompt schemas.}
    \label{fig:ablation-init}
\end{figure} 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.46\textwidth]{figs/comparison_ymc_lr.pdf}
    \caption{Results on GPT-3.5-Turbo with different optimization learning rates.}
    \label{fig:ablation-lr}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/comparison_baichuan.pdf}
        % \vspace{-5pt}
    \caption{Accuracy over 4 steps across 8 tasks on Baichuan2-Turbo.}
    \label{fig:ablation-baichuan}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/comparison_gpt-4.pdf}
        % \vspace{-5pt}
    \caption{Accuracy over 4 steps across 8 tasks on GPT-4.}
    \label{fig:ablation-gpt4}
\end{figure*}

\subsubsection{Different Initial Prompt Schemas}

Our method uses a meta-instruction to generate a prompt with four components: 
a) task type and description,
b) output format and constraints,
c) suggested reasoning process, and
d) professional tips.
We define:
\emph{Schema 4}: All four components
\emph{Schema 3}: First three components
\emph{Schema 2}: First two components
\emph{Schema 1}: Task type and description only (common in current techniques).
We vary the meta-instructions for these schemas and conduct four-step prompt optimization experiments on SST-2 and AG’s News to assess their impact on optimization.

As shown in Figure \ref{fig:ablation-init}, initial prompts from Schema 4 yield the highest evaluation results. In contrast, Schema 1 has the lowest metrics and often falls into suboptimal local minima, a common issue with current methods. This comparison validates our meta-instruction design and underscores that a high-quality initial prompt is crucial for quickly identifying the optimal prompt.

\subsubsection{Sensitivity to Learning Rate}





During the optimization phase, the learning rate $\eta$ controls the extent of sentence weight updates after each round. A higher $\eta$ results in significant updates and responsiveness to recent performance changes, while a lower $\eta$ promotes stability with gradual adjustments. This balance is crucial for navigating the trade-off between exploration and exploitation.

We conduct prompt optimization experiments on SST-2 and AG’s News within four steps, testing $\eta$ values from 0.01 to 0.1. As shown in Figure \ref{fig:ablation-lr}, $\eta=0.055$ and $\eta=0.07$ are the most and second most effective in accelerating optimization.

\subsubsection{Performance on Different LLMs}

As Table \ref{tab:main} indicates, APO is the best baseline method. Therefore, we compare our method with APO using Baichuan2 \cite{Yang2023Baichuan2O} and GPT-4 accessed via the APIs. We conduct prompt optimization experiments on eight NLP datasets across four optimization steps. % on the two additional LLMs.


Figure \ref{fig:ablation-baichuan} and \ref{fig:ablation-gpt4} illustrate the performance variation of both methods across different datasets as optimization steps progress. APO fails to converge within four steps and shows greater performance volatility compared to Baichuan2-Turbo and GPT-4. In contrast, our method demonstrates rapid convergence and strong optimization acceleration. Except for the generalizability to other models, we also find that stronger LLM can achieve more effective prompt optimization with our method.


\subsubsection{Performance on Specialized Domain-Specific Task}

To evaluate our method performance on specialized tasks that require domain knowledge, we conduct experiments on the Geometric Shapes task \cite{Suzgun2022ChallengingBT}, which involves interpreting SVG paths to determine the geometric figures they represent, a task that requires specific domain knowledge.

\begin{table}[!t]
  \centering
  \small
  \tabcolsep=4pt
  \begin{tabular}{l|c|c}
    \toprule
    \midrule
    \textbf{Model} & \textbf{APO} & \textbf{Ours} \\
    \midrule
    \textbf{GPT-3.5-Turbo} & 0.36 & 0.392 \\
    \midrule
    \textbf{GPT-4} & 0.448 & 0.488 \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy on Geometric Shapes task on GPT-3.5-Turbo and GPT-4.}
  \label{tab:Geo}
\end{table}

As shown in Table~\ref{tab:Geo}, our approach demonstrates consistent performance improvement over the best baseline APO, revealing the effectiveness of our method in specialized task.


\subsubsection{Results without Step Constraint}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.46\textwidth]{figs/comparison_gpt_our_apo.pdf}
    % \vspace{-3mm}
    \caption{Accuracy over 20 steps on GPT-3.5-Turbo.}
    \label{fig:ablation-20step}
\end{figure}

We report the results of prompt optimization with a maximum of 20 steps on two general NLU tasks.
As shown in Figure \ref{fig:ablation-20step}, the strongest baseline, APO, converges on the SST-2 task with slightly lower accuracy than our method. However, on the AG’s News task, APO’s performance fluctuates significantly and lags behind our method.
Thus, our method demonstrates superior performance and faster convergence compared to existing methods, even with fewer optimization steps.

\subsubsection{Computational Complexity}
Since the running time is related to the number of API calls and may be affected by the network condition, we mainly present the number of API calls, which is an important metric for cost comparison on black-box LLMs.

\begin{table}[!th]
  \centering
  \small
  \tabcolsep=5pt
  \renewcommand\arraystretch{1.2}
  \begin{tabular}{l|c|c}
    \toprule
    \midrule
    \textbf{Task} & \textbf{APO} & \textbf{Ours} \\
    \midrule
    \textbf{SST-2} & 12,520 & 1,708  \\
    \textbf{AG’s News} & 12,733 & 2,089 \\
    \textbf{TREC} & 9,739 & 1,486 \\
    \textbf{Subj} & 12,790 & 1,848 \\
    \midrule
    \textbf{Logical Five} & 9,631 & 1,512 \\
    \textbf{Hyperbaton} & 9,934 & 1,626 \\
    \textbf{Disambiguation} & 9,471 & 1,187 \\
    \textbf{Salient Translation} & 10,190 & 1,451 \\
    \textbf{Geometric Shapes} & 9,648 & 1,496 \\
    \midrule
    \textbf{Avg.} & 10,739 & 1,600 \\
    \bottomrule
  \end{tabular}
  \caption{API calls consumed on nine tasks on GPT-4.}
  \label{tab:API calls}
\end{table}

 We conduct our experiments with GPT-4 on nine tasks. As shown in Table~\ref{tab:API calls}, our method requires approximately 1/7 of the number of API calls compared to the strongest baseline method, APO.