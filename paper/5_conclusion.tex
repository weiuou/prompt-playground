\section{Conclusion}
In this paper, we addressed the issue of low convergence rates in gradient-free prompt optimization methods for LLMs. 
Our proposed dual-phase approach effectively accelerates prompt optimization by generating high-quality initial prompts and leveraging tuning experience to navigate the optimization process. 
Extensive experiments on several LLMs across diverse datasets demonstrated the superiority of our method in achieving satisfactory performance within few optimization steps. 
Our approach not only enhances the efficiency of prompt optimization but also improves the overall performance of LLMs in various NLP tasks. 
Future work will focus on further refining the optimization strategies and exploring their applications in more diverse and complex scenarios.