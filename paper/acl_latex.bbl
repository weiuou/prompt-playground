\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Auer et~al.(1995)Auer, Cesa-Bianchi, Freund, and Schapire}]{Auer1995GamblingIA}
Peter Auer, Nicol{\`o} Cesa-Bianchi, Yoav Freund, and Robert~E. Schapire. 1995.
\newblock \href {https://api.semanticscholar.org/CorpusID:267915948} {Gambling in a rigged casino: The adversarial multi-arm bandit problem}.
\newblock In \emph{IEEE Annual Symposium on Foundations of Computer Science}.

\bibitem[{Chen et~al.(2023)Chen, Chen, Goldstein, Huang, and Zhou}]{Chen2023InstructZeroEI}
Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:259075794} {Instructzero: Efficient instruction optimization for black-box large language models}.
\newblock \emph{ArXiv}, abs/2306.03082.

\bibitem[{Diao et~al.(2023{\natexlab{a}})Diao, Huang, Xu, Li, Yong, Zhou, and Zhang}]{diao2023blackbox}
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, LIN Yong, Xiao Zhou, and Tong Zhang. 2023{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=IvsGP7xRvm} {Black-box prompt learning for pre-trained language models}.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Diao et~al.(2023{\natexlab{b}})Diao, Wang, Lin, and Zhang}]{Diao2023ActivePW}
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023{\natexlab{b}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:257102707} {Active prompting with chain-of-thought for large language models}.
\newblock \emph{ArXiv}, abs/2302.12246.

\bibitem[{Gao et~al.(2021)Gao, Fisch, and Chen}]{Gao2021MakingPL}
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
\newblock \href {https://api.semanticscholar.org/CorpusID:229923710} {Making pre-trained language models better few-shot learners}.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}.

\bibitem[{Goyal et~al.(2022)Goyal, Li, and Durrett}]{Goyal2022NewsSA}
Tanya Goyal, Junyi~Jessy Li, and Greg Durrett. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:252532176} {News summarization and evaluation in the era of gpt-3}.
\newblock \emph{ArXiv}, abs/2209.12356.

\bibitem[{Guo et~al.(2023)Guo, Wang, Guo, Li, Song, Tan, Liu, Bian, Yang, University, and Research}]{Guo2023ConnectingLL}
Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu~Tan, Guoqing Liu, Jiang Bian, Yujiu Yang, Tsinghua University, and Microsoft Research. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:262012566} {Connecting large language models with evolutionary algorithms yields powerful prompt optimizers}.
\newblock \emph{ArXiv}, abs/2309.08532.

\bibitem[{Hsieh et~al.(2023)Hsieh, Si, Yu, and Dhillon}]{Hsieh2023AutomaticEO}
Cho-Jui Hsieh, Si~Si, Felix~X. Yu, and Inderjit~S. Dhillon. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:265281606} {Automatic engineering of long prompts}.
\newblock \emph{ArXiv}, abs/2311.10117.

\bibitem[{Li and Liang(2021)}]{Li2021PrefixTuningOC}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock \href {https://api.semanticscholar.org/CorpusID:230433941} {Prefix-tuning: Optimizing continuous prompts for generation}.
\newblock \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597.

\bibitem[{Lin et~al.(2024)Lin, Dai, Verma, Ng, Jaillet, and Low}]{Lin2024PromptOW}
Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Kian~Hsiang Low. 2024.
\newblock \href {https://api.semanticscholar.org/CorpusID:270063071} {Prompt optimization with human feedback}.
\newblock \emph{ArXiv}, abs/2405.17346.

\bibitem[{Lin et~al.(2023)Lin, Wu, Dai, Hu, Shu, Ng, Jaillet, and Low}]{Lin2023UseYI}
Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, and Bryan Kian~Hsiang Low. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:263620801} {Use your instinct: Instruction optimization using neural bandits coupled with transformers}.
\newblock \emph{ArXiv}, abs/2310.02905.

\bibitem[{Liu et~al.(2021{\natexlab{a}})Liu, Yuan, Fu, Jiang, Hayashi, and Neubig}]{Liu2021PretrainPA}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021{\natexlab{a}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:236493269} {Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing}.
\newblock \emph{ACM Computing Surveys}, 55:1 -- 35.

\bibitem[{Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{Liu2022PTuningPT}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:248780177} {P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks}.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}.

\bibitem[{Liu et~al.(2021{\natexlab{b}})Liu, Zheng, Du, Ding, Qian, Yang, and Tang}]{Liu2021GPTUT}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021{\natexlab{b}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:232269696} {Gpt understands, too}.
\newblock \emph{ArXiv}, abs/2103.10385.

\bibitem[{Pan et~al.(2023)Pan, Xing, Diao, Liu, Shum, Zhang, and Zhang}]{Pan2023PlumPL}
Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:265158095} {Plum: Prompt learning using metaheuristic}.
\newblock \emph{ArXiv}, abs/2311.08364.

\bibitem[{Pang and Lee(2004)}]{Pang2004ASE}
Bo~Pang and Lillian Lee. 2004.
\newblock \href {https://api.semanticscholar.org/CorpusID:388} {A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts}.
\newblock \emph{ArXiv}, cs.CL/0409058.

\bibitem[{Prasad et~al.(2022)Prasad, Hase, Zhou, and Bansal}]{Prasad2022GrIPSGE}
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:247447170} {Grips: Gradient-free, edit-based instruction search for prompting large language models}.
\newblock \emph{ArXiv}, abs/2203.07281.

\bibitem[{Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and Zeng}]{Pryzant2023AutomaticPO}
Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:258546785} {Automatic prompt optimization with "gradient descent" and beam search}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Qin et~al.(2024)Qin, Chen, Feng, Wu, Zhang, Li, Li, Che, and Yu}]{Qin2024LargeLM}
Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip~S. Yu. 2024.
\newblock \href {https://api.semanticscholar.org/CorpusID:269930153} {Large language models meet nlp: A survey}.
\newblock \emph{ArXiv}, abs/2405.12819.

\bibitem[{Reynolds and McDonell(2021)}]{Reynolds2021PromptPF}
Laria Reynolds and Kyle McDonell. 2021.
\newblock \href {https://api.semanticscholar.org/CorpusID:231925131} {Prompt programming for large language models: Beyond the few-shot paradigm}.
\newblock \emph{Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}.

\bibitem[{Shin et~al.(2020)Shin, Razeghi, IV, Wallace, and Singh}]{Shin2020ElicitingKF}
Taylor Shin, Yasaman Razeghi, Robert L~Logan IV, Eric Wallace, and Sameer Singh. 2020.
\newblock \href {https://api.semanticscholar.org/CorpusID:226222232} {Eliciting knowledge from language models using automatically generated prompts}.
\newblock \emph{ArXiv}, abs/2010.15980.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts}]{Socher2013RecursiveDM}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, A.~Ng, and Christopher Potts. 2013.
\newblock \href {https://api.semanticscholar.org/CorpusID:990233} {Recursive deep models for semantic compositionality over a sentiment treebank}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Suzgun et~al.(2022)Suzgun, Scales, Scharli, Gehrmann, Tay, Chung, Chowdhery, Le, hsin Chi, Zhou, and Wei}]{Suzgun2022ChallengingBT}
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~Huai hsin Chi, Denny Zhou, and Jason Wei. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:252917648} {Challenging big-bench tasks and whether chain-of-thought can solve them}.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}.

\bibitem[{Voorhees and Tice(2000)}]{Voorhees2000BuildingAQ}
Ellen~M. Voorhees and Dawn~M. Tice. 2000.
\newblock \href {https://api.semanticscholar.org/CorpusID:11465263} {Building a question answering test collection}.
\newblock In \emph{Annual International ACM SIGIR Conference on Research and Development in Information Retrieval}.

\bibitem[{Wang et~al.(2023)Wang, Li, Wang, Bai, Luo, Zhang, Jojic, Xing, and Hu}]{Wang2023PromptAgentSP}
Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric~P. Xing, and Zhiting Hu. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:264451925} {Promptagent: Strategic planning with language models enables expert-level prompt optimization}.
\newblock \emph{ArXiv}, abs/2310.16427.

\bibitem[{White et~al.(2023)White, Fu, Hays, Sandborn, Olea, Gilbert, Elnashar, Spencer-Smith, and Schmidt}]{White2023APP}
Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas~C. Schmidt. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:257079092} {A prompt pattern catalog to enhance prompt engineering with chatgpt}.
\newblock \emph{ArXiv}, abs/2302.11382.

\bibitem[{Yang et~al.(2023{\natexlab{a}})Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, Yang, Deng, Wang, Liu, Ai, Dong, Zhao, Xu, Sun, Zhang, Liu, Ji, Xie, Dai, Fang, Su, Song, Liu, Ru, Ma, Wang, Liu, Lin, Nie, Guo, Sun, Tao, Li, Li, Cheng, Chen, Zeng, Wang, Chen, Men, Yu, Pan, Shen, Wang, Li, Jiang, Gao, Zhang, Zhou, and Wu}]{Yang2023Baichuan2O}
Ai~Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023{\natexlab{a}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:261951743} {Baichuan 2: Open large-scale language models}.
\newblock \emph{ArXiv}, abs/2309.10305.

\bibitem[{Yang et~al.(2023{\natexlab{b}})Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{Yang2023LargeLM}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V. Le, Denny Zhou, and Xinyun Chen. 2023{\natexlab{b}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:261582296} {Large language models as optimizers}.
\newblock \emph{ArXiv}, abs/2309.03409.

\bibitem[{Yang et~al.(2024)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{yang2024large}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2024.
\newblock \href {https://openreview.net/forum?id=Bb4VGOWELI} {Large language models as optimizers}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Ye et~al.(2023)Ye, Axmed, Pryzant, and Khani}]{Ye2023PromptEA}
Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:265128994} {Prompt engineering a prompt engineer}.
\newblock \emph{ArXiv}, abs/2311.05661.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Liu, and Zhang}]{Zhang2023ExtractiveSV}
Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023{\natexlab{a}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:258048787} {Extractive summarization via chatgpt for faithful summary generation}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Zhang et~al.(2015)Zhang, Zhao, and LeCun}]{Zhang2015CharacterlevelCN}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun. 2015.
\newblock \href {https://api.semanticscholar.org/CorpusID:368182} {Character-level convolutional networks for text classification}.
\newblock In \emph{Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Wang, Yu, Xu, Iter, Zeng, Liu, Zhu, and Jiang}]{Zhang2023AutoInstructAI}
Zhihan Zhang, Shuo Wang, W.~Yu, Yichong Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, and Meng Jiang. 2023{\natexlab{b}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:264405621} {Auto-instruct: Automatic instruction generation and ranking for black-box language models}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Zheng et~al.(2023)Zheng, Mishra, Chen, Cheng, hsin Chi, Le, and Zhou}]{Zheng2023TakeAS}
Huaixiu~Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed~Huai hsin Chi, Quoc~V. Le, and Denny Zhou. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:263830368} {Take a step back: Evoking reasoning via abstraction in large language models}.
\newblock \emph{ArXiv}, abs/2310.06117.

\bibitem[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba}]{Zhou2022LargeLM}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.
\newblock \href {https://api.semanticscholar.org/CorpusID:253265328} {Large language models are human-level prompt engineers}.
\newblock \emph{ArXiv}, abs/2211.01910.

\end{thebibliography}
